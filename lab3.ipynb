{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d77230ee",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed8a00df369ce38f9743ed412973d72b",
     "grade": false,
     "grade_id": "cell-cbdf7d65ea58446b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "University of Zagreb\\\n",
    "Faculty of Electrical Engineering and Computing\n",
    "\n",
    "## Text Analysis and Retrieval 2021/2022\n",
    "https://www.fer.unizg.hr/predmet/apt/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee22e58",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1a7a9536ec842755316f7ccaa8971ac0",
     "grade": false,
     "grade_id": "cell-5e9c1e104dec0dd4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "------------------------------\n",
    "\n",
    "### Neural NLP\n",
    "### LAB 3\n",
    "\n",
    "\n",
    "*Version: 1.0*\n",
    "\n",
    "(c) 2022 Josip Jukić, Jan Šnajder\n",
    "\n",
    "Submission deadline: **May 29, 2022, 23:59 CET** \n",
    "\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6333437b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6233aca5abb9b3f8b6a7a6080fb096fb",
     "grade": false,
     "grade_id": "cell-b25d76fa7c847af2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "Hello visitor, this lab assignment consists of three parts. Your task boils down to filling out the missing parts of code and evaluating the cells. These parts are indicated by the \"YOUR CODE HERE\" template.\n",
    "\n",
    "Each subtask is supplemented by several tests that you can run. Apart from that, there are additional test that will be executed after submition. If your solution is valid and it passes all of the visible tests, there shouldn't be any problems with the additional tests.\n",
    "\n",
    "**IMPORTANT: Don't change the names of the predefined methods or random seeds**, because the tests won't execute properly.\n",
    "\n",
    "You're required to do this assignment **on your own**.\n",
    "\n",
    "If you stumble upon problems, please refer to josip.jukic@fer.hr for office hours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2336f31b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fd1654b2c7c8de8e1d1cfaeeee261ec2",
     "grade": false,
     "grade_id": "cell-150c23ae802b9522",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52503faa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9cf13460796faed6f774e768345186e9",
     "grade": false,
     "grade_id": "cell-95afad8333fec3bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1. Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28e247b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "563e4d2c420bc52c4d04db999317a738",
     "grade": false,
     "grade_id": "cell-7caecd650447b599",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Machine Translation (MT) is the task of automatically converting one natural language into another, preserving the meaning of the input text, and producing fluent text in the output language. While machine translation is one of the oldest subfields of artificial intelligence research, the recent shift towards large-scale empirical techniques has led to very significant improvements in translation quality.\n",
    "\n",
    "In this lab assignment, we will use pre-trained sequence-to-sequence models for machine translation. Additionally, we will consider two text generation strategies: greedy decoder and beam search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e673b2d3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b7a655abf3ffcbc41eae639cf60ac9c8",
     "grade": false,
     "grade_id": "cell-9c4e4d599e282ec6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### (a)\n",
    "\n",
    "Assuming that we are in possesion of a pre-trained model, we still need to figure out how exactly are we going to generate tokens. One of the simplest approaches is to retrieve the most probable token in each step.\n",
    "Implement `greedy_decoder` for language generation. Greedy method retrieves the index of the most probable token for each timestep in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80305ded",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "834924716c12bf23b38e72cd7c43d350",
     "grade": false,
     "grade_id": "cell-2bc8e6ac47722f0f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def greedy_decoder(array):\n",
    "    \"\"\"\n",
    "    Retrieves a 1D numpy array containing the index of the most\n",
    "    probable token for each row.\n",
    "\n",
    "    Arguments:\n",
    "        array: 2D np.array of token probabilities, where each row correspond to\n",
    "        a certain timestamp. For example, 10x5 array represents a sequence of 10\n",
    "        words over a vocabulary of 5 words.\n",
    "    \"\"\"\n",
    "    return np.argmax(array, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb28c515",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e800457ec180902671743277b02cd02",
     "grade": true,
     "grade_id": "cell-e6919b45595c671e",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "ex1a1 = np.array(\n",
    "    [\n",
    "        [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "        [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "        [0.1, 0.2, 0.3, 0.5, 0.4],\n",
    "        [0.2, 0.1, 0.3, 0.4, 0.5],\n",
    "        [0.3, 0.4, 0.5, 0.2, 0.1],\n",
    "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "        [0.1, 0.5, 0.3, 0.4, 0.2],\n",
    "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "    ]\n",
    ")\n",
    "\n",
    "sol1a1 = np.array([4, 0, 4, 0, 3, 4, 2, 0, 1, 0])\n",
    "\n",
    "assert (greedy_decoder(ex1a1) == sol1a1).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adca894a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1ae2ab666f197db3ecff1e7b440f622",
     "grade": false,
     "grade_id": "cell-37e0ec725ec6b044",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### (b)\n",
    "\n",
    "Greedy approach has the benefit that it is very fast, but the quality of the final output sequences may be far from optimal. Instead of greedily choosing the most likely next step as the sequence is constructed, the beam search expands all possible next steps and keeps the `k` most likely, where `k` is a parameter and controls the number of beams or parallel searches through the sequence of probabilities.\n",
    "\n",
    "The local beam search algorithm keeps track of `k` states rather than just one. It begins with `k` randomly generated states. At each step, all the successors of all `k` states are generated. If any one is a goal, the algorithm halts. Otherwise, it selects the k best successors from the complete list and repeats. This is illustrated in the figure below.\n",
    "\n",
    "Implement `beam_search_decoder`.\n",
    "\n",
    "![Beam search](img/beamsearch.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6f35d42",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6035a93ffd07d26b14e90ca24b418212",
     "grade": false,
     "grade_id": "cell-1daf6938107bcf6e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def beam_search_decoder(data, k):\n",
    "    \"\"\"\n",
    "    Retrieves top k sequences according to the beam search algorithm. The sequences\n",
    "    must be sorted in the descending order according to the sequence score.\n",
    "\n",
    "    Scores for sequences are computed as the sum of negative logarithms. For example, the\n",
    "    scores for sequence with probabilities [0.1, 0.3, 0.2] would be -log(0.1) - log(0.3) - log(0.2).\n",
    "\n",
    "    The result is returned as a sorted list of lists, where each list represents\n",
    "    one instance of the beam search output. For example, the list [[0, 1], [1, 2], [2, 3]] is\n",
    "    the result for k=3, where the first list [0, 1] (sequence of words from vocabulary at\n",
    "    position 0 and 1) is the output with the highest score, [1, 2]\n",
    "    has the second highest, and [2, 3] has the third highest score.\n",
    "\n",
    "\n",
    "    Arguments:\n",
    "        array: 2D np.array of token probabilities, where each row correspond to\n",
    "               a certain timestamp. For example, 10x5 array represents a sequence of 10\n",
    "               words over a vocabulary of 5 words.\n",
    "        k: number of beams\n",
    "    \"\"\"\n",
    "    all_paths = []\n",
    "    open_ = [([], 0)]\n",
    "    while open_ != []:\n",
    "        path, curr = open_.pop(0)\n",
    "\n",
    "        if len(path) == data.shape[0]:\n",
    "            path_copy = path[:]\n",
    "            path_copy.append(curr)\n",
    "            all_paths.append(path_copy[1:])\n",
    "            continue\n",
    "\n",
    "        k_beams = data[len(path)].argsort()[::-1][:k]\n",
    "\n",
    "        for beam in k_beams:\n",
    "            path_copy = path[:]\n",
    "            path_copy.append(curr)\n",
    "            open_.append((path_copy, beam))\n",
    "        \n",
    "    neg_logs = [(p, -np.sum(np.log(data[list(range(data.shape[0])), p]))) for p in all_paths]\n",
    "    neg_logs.sort(key=lambda x: x[1])\n",
    "    return [x[0] for x in neg_logs[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "077146a5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bc0dee599699e58abfc96cad12728c86",
     "grade": true,
     "grade_id": "cell-2752c4f710125130",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "ex1b1 = np.array(\n",
    "    [\n",
    "        [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "        [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "        [0.1, 0.2, 0.3, 0.5, 0.4],\n",
    "        [0.2, 0.1, 0.3, 0.4, 0.5],\n",
    "        [0.3, 0.4, 0.5, 0.2, 0.1],\n",
    "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "        [0.1, 0.5, 0.3, 0.4, 0.2],\n",
    "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "    ]\n",
    ")\n",
    "\n",
    "sol1b1 = np.array(\n",
    "    [\n",
    "        [4, 0, 4, 0, 3, 4, 2, 0, 1, 0],\n",
    "        [4, 0, 4, 0, 3, 4, 2, 0, 1, 1],\n",
    "        [4, 0, 4, 0, 3, 4, 2, 0, 3, 0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "assert (beam_search_decoder(ex1b1, 3) == sol1b1).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5026aa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b0cb3ec2ceac3f620300ee39d7086d92",
     "grade": false,
     "grade_id": "cell-be562f4995d3584e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### (c)\n",
    "\n",
    "Finally, let's employ a pre-trained sequence-to-sequence transformer from the [`hugggingface`](https://huggingface.co/) library. Specifically, we are going to use `mBART-large-50`, which can be employed in the [zero-shot](https://en.wikipedia.org/wiki/Zero-shot_learning) setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85def3c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "# Load the model and its corresponding tokenizer.\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57288b5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "07ad58e558933499e4b1b5e40188d73a",
     "grade": false,
     "grade_id": "cell-7247da3628b8c124",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Take a look at the example below to see how to use the pre-trained model in the zero-shot setup. We are translating from Finnish (fi_FI) to English (en_XX)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc099eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The morning goes by in bed, and the day ends in reverse.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi_text = \"Aamu kuluu aatellessa, päivä päätä käännellessä.\"\n",
    "tokenizer.src_lang = \"fi_FI\"\n",
    "encoded_ar = tokenizer(fi_text, return_tensors=\"pt\")\n",
    "generated_tokens = model.generate(\n",
    "    **encoded_ar, max_length=50, forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"]\n",
    ")\n",
    "tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebf1191",
   "metadata": {},
   "source": [
    "We can control the decoding algorithm by setting the `num_beams` parameter. If it is set to `None`, tokens will be decoded greedily. Additionally, we can control the sequence max length with `max_length` and early stopping with `early_stopping`. When set to True, `early_stopping` indicates that generation is finished when all beam hypotheses reached the EOS (end-of-sequence) token.\n",
    "\n",
    "Implement the `translate` method which generalizes translation using `mbart` to any source and target language supported by the model. If in doubt, refer to the previous example. Be sure to use all of the arguments, most of which are going to be forwarded to the model's `generate` method. `batch_decode` wraps the decoded tokens into a list, so don't forget to extract the first element from the list to get the actual string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12e4c8d0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ba3a8ae65344a27cf772c43ca00b2f6",
     "grade": false,
     "grade_id": "cell-30758986b31cf01b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def translate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    text,\n",
    "    src_lang,\n",
    "    tgt_lang=\"en_XX\",\n",
    "    num_beams=None,\n",
    "    max_length=50,\n",
    "    early_stopping=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Translates `text` from `src_lang` to `tgt_lang` and returns it as a string.\n",
    "    \"\"\"\n",
    "    tokenizer.src_lang = src_lang\n",
    "    encoded_ar = tokenizer(text, return_tensors=\"pt\")\n",
    "    generated_tokens = model.generate(\n",
    "        **encoded_ar, \n",
    "        max_length=max_length, \n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[tgt_lang], \n",
    "        early_stopping=early_stopping\n",
    "    )\n",
    "    return tokenizer.batch_decode(\n",
    "        generated_tokens, \n",
    "        skip_special_tokens=True, \n",
    "        num_beams=num_beams)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569b102b",
   "metadata": {},
   "source": [
    "Translate Finnish (fi_FI) and Portugese (pt_PT) texts to English (see the cell below) using the `translate` method. Is greedy decoding doing OK or is it better to use beam search? If beam search is better, which `k` seems to perform well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "155e2861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greed takes birds out of heaven and fish out of the sea.\n",
      "Greed takes birds out of heaven and fish out of the sea.\n",
      "Greed takes birds out of heaven and fish out of the sea.\n",
      "Greed takes birds out of heaven and fish out of the sea.\n",
      "Greed takes birds out of heaven and fish out of the sea.\n",
      "Time is money, said the unemployed when he sold his watch.\n",
      "Time is money, said the unemployed when he sold his watch.\n",
      "Time is money, said the unemployed when he sold his watch.\n",
      "Time is money, said the unemployed when he sold his watch.\n",
      "Time is money, said the unemployed when he sold his watch.\n",
      "But it's worth a penny in the hand of the two flying ones.\n",
      "But it's worth a penny in the hand of the two flying ones.\n",
      "But it's worth a penny in the hand of the two flying ones.\n",
      "But it's worth a penny in the hand of the two flying ones.\n",
      "But it's worth a penny in the hand of the two flying ones.\n",
      "Tell me who you are and I will tell you who you are.\n",
      "Tell me who you are and I will tell you who you are.\n",
      "Tell me who you are and I will tell you who you are.\n",
      "Tell me who you are and I will tell you who you are.\n",
      "Tell me who you are and I will tell you who you are.\n"
     ]
    }
   ],
   "source": [
    "fi_text_1 = \"Ahneus vie linnut taivaalta ja kalat merestä.\"\n",
    "fi_text_2 = \"Aika on rahaa, sanoi työtön kun kellonsa myi.\"\n",
    "\n",
    "pt_text_1 = \"Mais vale um pássaro na mão do que dois voando.\"\n",
    "pt_text_2 = \"Diz-me com quem andas e eu te direi quem és.\"\n",
    "\n",
    "print(translate(model, tokenizer, fi_text_1, \"fi_FI\"))\n",
    "print(translate(model, tokenizer, fi_text_1, \"fi_FI\", num_beams=5))\n",
    "print(translate(model, tokenizer, fi_text_1, \"fi_FI\", num_beams=20))\n",
    "print(translate(model, tokenizer, fi_text_1, \"fi_FI\", num_beams=50))\n",
    "print(translate(model, tokenizer, fi_text_1, \"fi_FI\", num_beams=100))\n",
    "\n",
    "\n",
    "print(translate(model, tokenizer, fi_text_2, \"fi_FI\"))\n",
    "print(translate(model, tokenizer, fi_text_2, \"fi_FI\", num_beams=5))\n",
    "print(translate(model, tokenizer, fi_text_2, \"fi_FI\", num_beams=20))\n",
    "print(translate(model, tokenizer, fi_text_2, \"fi_FI\", num_beams=50))\n",
    "print(translate(model, tokenizer, fi_text_2, \"fi_FI\", num_beams=100))\n",
    "\n",
    "\n",
    "print(translate(model, tokenizer, pt_text_1, \"pt_PT\"))\n",
    "print(translate(model, tokenizer, pt_text_1, \"pt_PT\", num_beams=5))\n",
    "print(translate(model, tokenizer, pt_text_1, \"pt_PT\", num_beams=20))\n",
    "print(translate(model, tokenizer, pt_text_1, \"pt_PT\", num_beams=50))\n",
    "print(translate(model, tokenizer, pt_text_1, \"pt_PT\", num_beams=100))\n",
    "\n",
    "\n",
    "print(translate(model, tokenizer, pt_text_2, \"pt_PT\"))\n",
    "print(translate(model, tokenizer, pt_text_2, \"pt_PT\", num_beams=5))\n",
    "print(translate(model, tokenizer, pt_text_2, \"pt_PT\", num_beams=20))\n",
    "print(translate(model, tokenizer, pt_text_2, \"pt_PT\", num_beams=50))\n",
    "print(translate(model, tokenizer, pt_text_2, \"pt_PT\", num_beams=100))\n",
    "\n",
    "# all the translations are the same when using the greedy approach and the beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e7fe29b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f6fc0d5e39579a4429678e4c7312109f",
     "grade": true,
     "grade_id": "cell-00bb6ec8ed77cc00",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "ex1c1 = \"Anfangen ist leicht, Beharren ist Kunst\"\n",
    "assert translate(model, tokenizer, ex1c1, \"de_DE\") == \"Starting is easy, barking is art\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
